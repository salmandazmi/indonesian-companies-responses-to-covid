{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library - Import & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at ./Text Summarization/model/t5-base-indonesian-summarization-cased/ and are newly initialized: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import newspaper\n",
    "import csv, io\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from docx import Document\n",
    "from docx.text.paragraph import Paragraph\n",
    "from docx.shared import Cm, Pt\n",
    "\n",
    "from newspaper import Article\n",
    "from newspaper import Config\n",
    "\n",
    "# Newspaper3k config\n",
    "config = Config()\n",
    "config.request_timeout = 120\n",
    "LINK_PATH = 'Text Summarization\\data\\link_swa.csv'\n",
    "\n",
    "# Huggingface's Transformers config\n",
    "MODEL_PATH = \"./Text Summarization/model/t5-base-indonesian-summarization-cased/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Load downloaded news data\n",
    "DATA_PATH = r'Text Summarization\\data\\news_swa.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_download(first, last, LINK_PATH, DATA_PATH):\n",
    "    link = []\n",
    "    \n",
    "    # Load links\n",
    "    with open(LINK_PATH, 'r', ) as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        link = list(reader)\n",
    "        link = link[first:last+1]\n",
    "        article_list = []\n",
    "        i=1\n",
    "    \n",
    "    # Download news text\n",
    "    for url in link:\n",
    "        a = Article(url[1], language='id', config=config)\n",
    "        a.download()\n",
    "        a.parse()\n",
    "        a.text = a.text[(a.text.find('\\n')+1):]\n",
    "        a.text = a.text[1:]\n",
    "        a.nlp()\n",
    "        #a.text = a.text.replace('#','')\n",
    "        a.summary = a.summary.replace('\\n\\n',' ').replace('\\n',' ')\n",
    "        with open(DATA_PATH, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "            spamwriter = csv.writer(csvfile, delimiter=',',quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "            spamwriter.writerow([url[0], a.title, a.text, a.publish_date, a.url, a.summary])\n",
    "        print(\"News ID \" + str(url[0]) + \" downloaded\")\n",
    "        article_list.append(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate chunks of text / paragraph <= 512 tokens\n",
    "def nest_paragraph(document):\n",
    "    nested = []\n",
    "    sent = []\n",
    "    length = 0\n",
    "    \n",
    "    paragraph = document.split('\\n\\n')\n",
    "    paragraph = [ x for x in paragraph if \"www.swa.co.id\" not in x ]\n",
    "    paragraph = [ x for x in paragraph if \"Editor\" not in x ]\n",
    "    \n",
    "    for elem in paragraph:\n",
    "        length += len(elem)\n",
    "        if length < 1024:\n",
    "            sent.append(elem)\n",
    "        else:\n",
    "            nested.append(sent)\n",
    "            sent = [elem]\n",
    "            length = len(elem)\n",
    "\n",
    "    if sent:\n",
    "        nested.append(sent)\n",
    "        \n",
    "    nested_paragraph = []\n",
    "    \n",
    "    for elem in nested:\n",
    "        elem = ' '.join(elem)\n",
    "        nested_paragraph.append(elem)\n",
    "    return nested_paragraph\n",
    "\n",
    "# Generate summary on text with <= 512 tokens\n",
    "def generate_summary(nested_paragraph):\n",
    "    summaries = []\n",
    "    info = {\n",
    "        'text_chars': [],\n",
    "        'text_words': [],\n",
    "        'text_tokens': [],\n",
    "        'sum_chars': [],\n",
    "        'sum_words': [],\n",
    "        'sum_tokens': []\n",
    "    }  \n",
    "   \n",
    "    for elem in nested_paragraph:\n",
    "        # T5 uses a max_length of 512 so we cut the article to 512 tokens.\n",
    "        inputs = tokenizer.encode(\"summarize: \" + elem, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "        summary = [tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)]\n",
    "    \n",
    "        summaries.append(summary)\n",
    "        info['text_chars'].append(len(elem))\n",
    "        info['text_words'].append(len(elem.split(' ')))\n",
    "        info['text_tokens'].append(len(inputs[0]))\n",
    "        info['sum_chars'].append(len(summary[0]))\n",
    "        info['sum_words'].append(len(summary[0].split(' ')))\n",
    "        info['sum_tokens'].append(len(outputs[0]))\n",
    "    \n",
    "    summaries = [sentence for sublist in summaries for sentence in sublist]\n",
    "    infos = \"Text : \\n\" + str(round(sum(info['text_chars'])/len(info['text_chars']))) + \" characters(avg), \\n\" + str(round(sum(info['text_words'])/len(info['text_words']))) + \" words(avg), \\n\" + str(round(sum(info['text_tokens'])/len(info['text_tokens']))) + \" tokens(avg), \\n\" + str(max(info['text_tokens'])) + \" tokens(max), \\n\" + str(min(info['text_tokens'])) + \" tokens (min). \" + \"\\n\\n\" + \"Summary : \" + str(round(sum(info['sum_chars'])/len(info['sum_chars']))) + \" characters(avg), \\n\" + str(round(sum(info['sum_words'])/len(info['sum_words']))) + \" words(avg), \\n\" + str(round(sum(info['sum_tokens'])/len(info['sum_tokens']))) + \" tokens(avg), \\n\" + str(max(info['sum_tokens'])) + \" tokens(max), \\n\" + str(min(info['sum_tokens'])) + \" tokens (min).\"\n",
    "#     infos = \"Text : \\n\" + str(round(sum(info['text_chars']))) + \" characters, \\n\" + str(round(sum(info['text_words']))) + \" words, \\n\" + str(round(sum(info['text_tokens']))) + \" tokens, \\n\" + str(max(info['text_tokens'])) + \" tokens(max), \\n\" + str(min(info['text_tokens'])) + \" tokens (min). \" + \"\\n\\n\" + \"Summary : \" + str(round(sum(info['sum_chars']))) + \" characters, \\n\" + str(round(sum(info['sum_words']))) + \" words, \\n\" + str(round(sum(info['sum_tokens']))) + \" tokens, \\n\" + str(max(info['sum_tokens'])) + \" tokens(max), \\n\" + str(min(info['sum_tokens'])) + \" tokens (min).\"\n",
    "    return [summaries, infos]\n",
    "\n",
    "# Combine all sub-summary result\n",
    "def summarization(document):\n",
    "    nest = nest_paragraph(document)\n",
    "    result = generate_summary(nest)\n",
    "    separator = ' '\n",
    "    summary_result = separator.join(result[0])\n",
    "    infos = result[1]\n",
    "    return [summary_result, infos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize All News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize news data from id a to b => write in docx file\n",
    "def news_summarization(first, last, DATA_PATH):\n",
    "    data = pd.read_csv(DATA_PATH, encoding= 'utf-8')\n",
    "    data = data.iloc[first-1:last]\n",
    "    \n",
    "    #docx writer config\n",
    "    SUMMARY_PATH = 'Text Summarization\\summary\\summary_swa_final_'+ str(first) + '-' +str(last) +'.docx'\n",
    "    \n",
    "    word_document = Document()\n",
    "    table = word_document.add_table(0, 0) # we add rows iteratively\n",
    "    table.style = 'Table Grid'\n",
    "    first_column_width = 1\n",
    "    second_column_width = 3\n",
    "    third_column_width = 5\n",
    "    fourth_column_width = 5\n",
    "    fifth_column_width = 1\n",
    "    table.add_column(Cm(first_column_width))\n",
    "    table.add_column(Cm(second_column_width))\n",
    "    table.add_column(Cm(third_column_width))\n",
    "    table.add_column(Cm(fourth_column_width))\n",
    "    table.add_column(Cm(fifth_column_width))\n",
    "    table.add_row()\n",
    "    row = table.rows[0]\n",
    "    row.cells[0].text = str('Num')\n",
    "    row.cells[1].text = str('Title')\n",
    "    row.cells[2].text = str('Summary')\n",
    "    row.cells[3].text = str('Content')\n",
    "    row.cells[4].text = str('Info')\n",
    "    \n",
    "    for i, elem in enumerate(data['Content'].isnull()):\n",
    "        if elem == False:\n",
    "            summarization_result = summarization(data['Content'].iloc[i])\n",
    "            summarization_result[0] = summarization_result[0].replace('\\%', '%')\n",
    "            table.add_row()\n",
    "            row = table.rows[i+1]\n",
    "            row.cells[0].text = str(data['Num'].iloc[i])\n",
    "            row.cells[1].text = str(data['Title'].iloc[i])\n",
    "            row.cells[2].text = str(summarization_result[0])\n",
    "            row.cells[3].text = str(data['Content'].iloc[i])\n",
    "            row.cells[4].text = str(summarization_result[1])\n",
    "            word_document.save(SUMMARY_PATH)\n",
    "            print(\"News ID \" + str(data['Num'].iloc[i]) + \" summarized\")\n",
    "        else:\n",
    "            table.add_row()\n",
    "            row = table.rows[i+1]\n",
    "            row.cells[0].text = str(data['Num'].iloc[i])\n",
    "            row.cells[1].text = str(data['Title'].iloc[i])\n",
    "            row.cells[2].text = str('-')\n",
    "            row.cells[3].text = str(data['Content'].iloc[i])\n",
    "            row.cells[4].text = str(summarization_result[1])\n",
    "            word_document.save(SUMMARY_PATH)\n",
    "            print(\"News ID \" + str(data['Num'].iloc[i]) + \" summarized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News ID 1 downloaded\n",
      "News ID 2 downloaded\n",
      "News ID 3 downloaded\n",
      "News ID 4 downloaded\n",
      "News ID 5 downloaded\n"
     ]
    }
   ],
   "source": [
    "LINK_PATH = 'Text Summarization\\data\\link_swa.csv'\n",
    "DATA_PATH = r'Text Summarization\\data\\news_swa_owewoe.csv'\n",
    "\n",
    "# Download news item number a to b\n",
    "news_download(1, 5, LINK_PATH, DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News ID 500 summarized\n",
      "News ID 501 summarized\n",
      "News ID 502 summarized\n",
      "News ID 503 summarized\n",
      "News ID 504 summarized\n",
      "News ID 505 summarized\n",
      "News ID 506 summarized\n",
      "News ID 507 summarized\n",
      "News ID 508 summarized\n",
      "News ID 509 summarized\n",
      "News ID 510 summarized\n",
      "News ID 511 summarized\n",
      "News ID 512 summarized\n",
      "News ID 513 summarized\n",
      "News ID 514 summarized\n",
      "News ID 515 summarized\n",
      "News ID 516 summarized\n",
      "News ID 517 summarized\n",
      "News ID 518 summarized\n",
      "News ID 519 summarized\n",
      "News ID 520 summarized\n",
      "News ID 521 summarized\n",
      "News ID 522 summarized\n",
      "News ID 523 summarized\n",
      "News ID 524 summarized\n",
      "News ID 525 summarized\n",
      "News ID 526 summarized\n",
      "News ID 527 summarized\n",
      "News ID 528 summarized\n",
      "News ID 529 summarized\n",
      "News ID 530 summarized\n",
      "News ID 531 summarized\n",
      "News ID 532 summarized\n",
      "News ID 533 summarized\n",
      "News ID 534 summarized\n",
      "News ID 535 summarized\n",
      "News ID 536 summarized\n",
      "News ID 537 summarized\n",
      "News ID 538 summarized\n",
      "News ID 539 summarized\n",
      "News ID 540 summarized\n",
      "News ID 541 summarized\n",
      "News ID 542 summarized\n",
      "News ID 543 summarized\n",
      "News ID 544 summarized\n",
      "News ID 545 summarized\n",
      "News ID 546 summarized\n",
      "News ID 547 summarized\n",
      "News ID 548 summarized\n",
      "News ID 549 summarized\n",
      "News ID 550 summarized\n",
      "News ID 551 summarized\n",
      "News ID 552 summarized\n",
      "News ID 553 summarized\n",
      "News ID 554 summarized\n",
      "News ID 555 summarized\n",
      "News ID 556 summarized\n",
      "News ID 557 summarized\n",
      "News ID 558 summarized\n",
      "News ID 559 summarized\n",
      "News ID 560 summarized\n",
      "News ID 561 summarized\n",
      "News ID 562 summarized\n",
      "News ID 563 summarized\n",
      "News ID 564 summarized\n",
      "News ID 565 summarized\n",
      "News ID 566 summarized\n",
      "News ID 567 summarized\n",
      "News ID 568 summarized\n",
      "News ID 569 summarized\n",
      "News ID 570 summarized\n",
      "News ID 571 summarized\n",
      "News ID 572 summarized\n",
      "News ID 573 summarized\n",
      "News ID 574 summarized\n",
      "News ID 575 summarized\n",
      "News ID 576 summarized\n",
      "News ID 577 summarized\n",
      "News ID 578 summarized\n",
      "News ID 579 summarized\n",
      "News ID 580 summarized\n",
      "News ID 581 summarized\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = r'Text Summarization\\data\\news_swa.csv'\n",
    "\n",
    "# Summary news item number a to b\n",
    "news_summarization(500, 581, DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info - Pemilihan Ringkasan Berita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah berita terpilih = 225\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(DATA_PATH, encoding= 'utf-8')\n",
    "print(\"Jumlah berita terpilih = \" + str((data.Select == 'V').sum()-4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
